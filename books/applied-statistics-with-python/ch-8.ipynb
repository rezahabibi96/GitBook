{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c5b686",
   "metadata": {},
   "source": [
    "# Correlation and Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b0b83a",
   "metadata": {
    "tags": [
     "hide-cell",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "base_path = (\n",
    "    \"https://raw.githubusercontent.com/rezahabibi96/GitBook/refs/heads/main/\"\n",
    "    \"books/applied-statistics-with-python/.resources\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c4a9c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.utils import resample\n",
    "from scipy.stats import norm, binom, chi2, chisquare, chi2_contingency, expon, shapiro, t, wilcoxon\n",
    "from statsmodels.stats.power import TTestPower, TTestIndPower\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d8501c",
   "metadata": {},
   "source": [
    "In this chapter, we introduce methods for determining whether there is a linear correlation, or association, between two numerical variables. In the case it exists, we learn how to derive a linear model and how to use it for prediction, which is very important in applications of many different fields from Business and Finance to Biology and Medicine.\n",
    "For example:\n",
    "* Based on income, consumer debt, and several other predictors about a person, we might be interested in predicting a person's credit rating.\n",
    "* We might be interested in predicting growth in sales of a company based on current economic conditions and its previous year's performance.\n",
    "* In Medicine, it would be important to predict patients' blood pressure based on health measurements and the dosage of a new drug.\n",
    "\n",
    "In fairness, the more interesting examples above have several predictors, which is called multiple regression and is taken up in detail in later chapters. In this chapter, we study correlation in general, but the linear models are limited to a single predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2af5e4a",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8446fd",
   "metadata": {},
   "source": [
    "As an example, let's return to the file `MHEALTH.csv` mentioned before in chapters 1 and 2. It contains many health measurements for 40 men. Let's investigate weight (WT) dependence on the waist size (WAIST).\n",
    "\n",
    "Whenever you plan a correlation/regression study, you *must always start with a scatterplot* as shown in the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c9d33",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed6d97ab",
   "metadata": {},
   "source": [
    "There is a fair amount of variation, but there seems to be an overall linear trend with a positive slope. Not surprisingly, a man with a larger waist size has a higher weight.\n",
    "\n",
    "The numerical measure of the strength of this *linear* trend is given by the **Pearson correlation coefficient**:\n",
    "\n",
    "$$ r = \\frac{1}{n-1} \\sum_{i=1}^n (z_x)_i \\cdot (z_y)i = \\frac{1}{n-1} \\sum{i=1}^n \\frac{x_i - \\bar{x}}{s_x} \\cdot \\frac{y_i - \\bar{y}}{s_y} $$\n",
    "\n",
    "where $n$ is the number of data point pairs. The `scipy.stats` library has an effective function to perform this rather tedious computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b70567",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cb49a0c",
   "metadata": {},
   "source": [
    "Note that because the standardized z-scores are used to define $r$, it is a dimensionless number that does not depend on units of measurement of either variable.\n",
    "For example, assume that $x$ is the square footage ($ft^2$) of an apartment, and $y$ is its price in dollars. If we change $x$ to square meters and/or $y$ to thousands of dollars, the correlation coefficients would not change.\n",
    "Also, $x$ and $y$ come symmetrically into the $r$ formula, so they can be interchanged.\n",
    "\n",
    "The correlation coefficient is always between $-1 \\le r \\le 1$, and the figure below illustrates different sample scatterplots and their correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f08d980",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8d8df73",
   "metadata": {},
   "source": [
    "A set of heuristic bounds for the correlation coefficient could be defined as follows:\n",
    "1. $0 \\le |r| \\le 0.3$ — no correlation to weak correlation\n",
    "2. $0.3 \\le |r| \\le 0.6$ — weak correlation to moderate correlation\n",
    "3. $0.6 \\le |r| \\le 1$ — moderate correlation to strong correlation\n",
    "\n",
    "These bounds are imprecise and very much depend on the field of study. In a science or an engineering experiment, when the variables are related by a Physics law, a correlation must be very high, close to $-1$ or $1$. On the other hand, in business, economics, medicine, etc., with all the variability in human behavior, even a correlation of $0.6$ could be considered strong.\n",
    "In the figure above, top to bottom, left to right, the correlation coefficient decreases from perfect positive to strong, moderate, weak, no correlation, and similarly for negative correlations.\n",
    "Note that Pearson correlation coefficient $r$ only quantifies **linear** correlation. It **can't** describe any **non-linear** relationship. The $r$ could still be computed, but it does not make much sense as shown in the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d82834",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e0b7e2f",
   "metadata": {},
   "source": [
    "The formula for the correlation coefficient is\n",
    "$ r = \\frac{1}{n-1} \\sum_{i=1}^n (z_x)_i \\cdot (z_y)_i $\n",
    "and the figure below can help understand the sign and magnitude of the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d33e0",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42f746c4",
   "metadata": {},
   "source": [
    "For the positive slope (uphill) line, $z_x$ and $z_y$ are either both $> 0$ or both $< 0$; therefore the $\\sum z_x \\cdot z_y$ is positive and large. Analogously, for the negative slope (downhill) line, $z_x$ and $z_y$ have opposite signs, so $\\sum z_x \\cdot z_y$ is negative and large. For the points in the middle plot, which are all over the place, $z_x \\cdot z_y$ have different signs and cancel each other out, so $\\sum z_x \\cdot z_y$ is close to 0. This sum is divided by $n-1$ to average, which leads to bounds $-1 \\le r \\le 1$.\n",
    "\n",
    "The sample correlation coefficient $r$ is an approximation of the true population correlation coefficient $\\rho$. The hypothesis test for $\\rho$ is:\n",
    "\n",
    "$H_0$: $\\rho = 0$ — no linear correlation\n",
    "\n",
    "$H_1$: $\\rho \\ne 0$ — there is a linear correlation\n",
    "\n",
    "The hypothesis is tested with t-distribution with $df = n-2$ degrees of freedom:\n",
    "\n",
    "$$ SE = \\sqrt{\\frac{1-r^2}{n-2}} $$\n",
    "\n",
    "$$ t = \\frac{(\\text{point estimate}) - (\\text{null value})}{SE} = \\frac{r - 0}{SE} = \\frac{r}{SE} $$\n",
    "\n",
    "Note that the Pearson's $r$ hypothesis test is a parametric test, and the data must satisfy the usual assumptions that both variables follow normal distributions. At least no outliers should dramatically change $r$. Such outliers are influential observations, which will be discussed later in this chapter.\n",
    "\n",
    "For example, for WT and WAIST size in the MHEALTH data file, the above formulas give:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7e4485",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae51bf5f",
   "metadata": {},
   "source": [
    "Therefore, WAIST and WT are strongly positively linearly correlated. Also, $p$-value $< 0.05$, so we can reject $H_0: \\rho = 0$ (no linear correlation) to claim that this correlation is significant.\n",
    "\n",
    "On the other hand, for WT and PULSE, there is practically no correlation and $p$-value $> 0.05$ (non-significant):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5261f8b2",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0abca140",
   "metadata": {},
   "source": [
    "Note that the strength of the correlation is given by the magnitude of $r$, while the significance is determined by the $p$-value of the corresponding t-test. They are related, but not the same. Say, for a large data set, even a weak correlation coefficient can be statistically significant.\n",
    "\n",
    "Many possible pairwise correlation coefficients exist in the case of several interrelated numerical variables. The code below gives an efficient way to obtain all the pairwise correlations at once and have their visual representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b943b",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5cc417",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "266ed3fa",
   "metadata": {},
   "source": [
    "Therefore, WT is weakly correlated with AGE, not correlated with CHOL (cholesterol), moderately correlated with HT (height), and strongly positively correlated with BMI. We can check individual significance with $p$-value, for example for WT and HT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498daa3d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3501192",
   "metadata": {},
   "source": [
    "Moreover, the square of $r$ — $r^2$ — is called the **coefficient of determination** and has an important meaning. It is the proportion of the variation in the response variable $y$ that is explained by the linear relationship between $x$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90049765",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fd145a8",
   "metadata": {},
   "source": [
    "For example, for the relationship between HT and WT, $r^2 = 0.5222^2 = 0.273$. Therefore, 27.3% of variation in WT can be explained by the linear relationship with HT.\n",
    "\n",
    "On the other hand, for the relationship between WT and PULSE, $r^2 = 0.0562^2 = 0.0031$. Therefore, only 0.31% of variation in WT can be explained with the linear relationship with PULSE (no relationship).\n",
    "\n",
    "To repeat the point that was made in the beginning, **correlation does NOT imply causation**. Only a randomized experiment can be used to assess the causality!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bcd08f",
   "metadata": {},
   "source": [
    "### Spearman Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4e9747",
   "metadata": {},
   "source": [
    "In the case when the samples are small and/or skewed, the normality cannot be assumed, and non-parametric tests that make no such assumptions are in order. Spearman rank-order correlation can evaluate the monotonic relationship between two continuous or ordinal variables. In such a relationship, the variables tend to change together, but not necessarily at a constant rate.\n",
    "Much like other non-parametric tests (Wilcoxon, Mann-Whitney, etc.), the Spearman correlation coefficient is based on the ranked values rather than the raw data. As a ranking, it can also be naturally used for ordinal variables like the Likert Scale: \n",
    "(1 — Strongly Disagree,\n",
    "2 — Disagree,\n",
    "3 — Neutral,\n",
    "4 — Agree,\n",
    "5 — Strongly Agree)\n",
    "\n",
    "As an example, consider a data file on a possible correlation between the number of medications a person took in the past week (discrete counting variable, not normally distributed) and the score on a standard memory recall test. We expect that the greater the number of medications, the worse the patients' memory will be; therefore a one-tailed test is in order. The data set is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89955427",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98db9836",
   "metadata": {},
   "source": [
    "The correlation coefficient is negative (as predicted), but it is weak and not statistically significant at any acceptable level of significance. Note that we have to divide the $p$-value which we obtained by 2 (one-sided test), but it is still way more than 0.05. Thus, the number of medications was not significantly related to memory performance.\n",
    "\n",
    "Kendall's rank correlation tests the similarities in the ordering of data ranked by quantities. Pearson and Spearman use the individual observations as the basis of the correlation, while Kendall's correlation coefficient uses pairs of observations. It determines the strength of association based on the pattern of concordance and discordance between the pairs. For the medication problem above, the conclusions are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee676acc",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2ae6bfc",
   "metadata": {},
   "source": [
    "### Partial Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc718d",
   "metadata": {},
   "source": [
    "Partial correlation is the correlation between two variables while controlling for a third (or more) variable.\n",
    "Consider an example data set of `Income` and `Performance` scores at a job together with `Experience` in years. As we can see in the correlation matrix figure below, the correlations computed without partialling anything out are very strong between all three variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff0e842",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7877ffc",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b3db3",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb0a350",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0df52f5e",
   "metadata": {},
   "source": [
    "In the above output, the years of `Experience` have been covaried (partialled out). The correlation between Income and Performance score has been reduced considerably to less than half of the original one, and the $p$-value $> 0.05$, so it is not significant anymore.\n",
    "It removed that part of the correlation between `Income` and `Performance` score which was due to years of `Experience`, and there is not much correlation left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e55c7bc",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfa3dd0a",
   "metadata": {},
   "source": [
    "The correlation above has been reduced to a moderate 0.61, but it is still significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bf0f22",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fbbf087",
   "metadata": {},
   "source": [
    "The correlation above is not significant anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c15b050",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27c981ab",
   "metadata": {},
   "source": [
    "We can illustrate this idea using the diagram above showing overlapping circles for each of our three variables. It is not mathematically accurate; the circles are exaggerated to understand the idea.\n",
    "The shared variance between `Income` and `Performance` is represented by the areas a and b.\n",
    "The `Experience` is related (shares variance) with both `Income` and `Performance` in areas c, b, and d, respectively.\n",
    "Area a shows the unique variance of `Income` and `Performance`.\n",
    "Area b represents the part of the relationship between `Income` and `Performance` which is due to Experience.\n",
    "If we remove the influence of Experience (area b), the correlation between `Income` and `Performance` is considerably reduced (area a only).\n",
    "\n",
    "A medical example of partial correlation might be a relationship between symptoms and quality of life, with depression partialled out (covaried). If the correlation between symptoms and quality of life is considerably reduced after partialling out depression, it can be concluded that a part of the relationship between symptoms and quality of life is due to depression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec436b",
   "metadata": {},
   "source": [
    "## Least Squares Regression Line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1aaad6",
   "metadata": {},
   "source": [
    "Given that there is a strong correlation between `WT` and `WAIST`, the observations follow a linear model. What is the equation of the best line? We denote it as:\n",
    "\n",
    "$$ \\hat{y} = E(Y \\mid X = x) = b_0 + b_1 x $$\n",
    "\n",
    "To find the best line, a **residual** for each data point $i = 1\\dots n$ is defined as the vertical distance between the actual $y$-value $y_i$ of the data point and its prediction by the line $\\hat{y}_i$:\n",
    "\n",
    "$$ e_i = y_i - \\hat{y}_i = y_i - (b_0 + b_1 \\cdot x_i) $$\n",
    "\n",
    "The figure below shows data with the best regression line on the left side and residuals on the right side using 'sns.regplot()' and `sns.residplot()`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4904a49",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "548541e6",
   "metadata": {},
   "source": [
    "The residuals seem to be randomly positive and negative around $y = 0$, so the line fits the data well. The positive and negative residuals would cancel each other in a sum. Instead, a sum of squared residuals: $e_1^2 + e_2^2 + \\dots + e_n^2$ is minimized (**least squares regression line**). The idea is analogous to defining the standard deviation using the sum of squared deviations from the mean:\n",
    "$ s = \\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n-1}} $\n",
    "The minimization is done using Multivariable Calculus. The slope is:\n",
    "\n",
    "$$ b_1 = \\frac{s_y \\cdot r}{s_x} $$\n",
    "\n",
    "where $r$ is the correlation coefficient, and $s_x$ and $s_y$ are standard deviations.\n",
    "The proportionality and positivity of standard deviations imply that the sign of the slope of the line is the same as the correlation coefficient $r$ sign. Also, it is shown that the means point $(\\bar{x}, \\bar{y})$ is always on the regression line. Therefore, the point-slope form of the equation of regression line is:\n",
    "\n",
    "$$ y - \\bar{y} = b_1 (x - \\bar{x}) \\implies y = \\bar{y} - b_1 \\bar{x} + b_1 x $$\n",
    "\n",
    "Therefore, the y-intercept is:\n",
    "\n",
    "$$ b_0 = \\bar{y} - b_1 \\bar{x} $$\n",
    "\n",
    "Let's illustrate these formulas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81177942",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "171b8b8b",
   "metadata": {},
   "source": [
    "Based on the code above, the y-intercept is $b_0 = -44.08$, and the slope is $m = b_1 = 2.37$. Therefore:\n",
    "\n",
    "$$ \\hat{y} = b_0 + b_1 x = -44.08 + 2.37 \\cdot x $$\n",
    "\n",
    "or in terms of the original variables:\n",
    "\n",
    "$$ \\hat{WT} = -44.08 + 2.37 \\cdot WAIST $$\n",
    "\n",
    "A slope is a rise over run; therefore here, for each 1 cm increase in `WAIST`, you are expected to gain **on average** 2.37 lb of `WT`. Note that for the linear model, it matters that **explanatory (independent) variabl*e* `WAIST` explains **response (dependent) variable** `WT`, while for correlation it did not matter.\n",
    "\n",
    "Let's find the residual for a given man with `WAIST` size of 95 cm and weight of 181 lb (data point $(95, 181)$):\n",
    "\n",
    "$$ \\hat{y} = -44.08 + 2.37 \\cdot 95 \\approx 181.37 $$\n",
    "\n",
    "Therefore, the residual is: \n",
    "\n",
    "$$ e = y - \\hat{y} = 181 - 181.37 = -0.37 < 0 $$\n",
    "\n",
    "Now, for a different man with `WAIST` size of 75 cm and weight of 170, the residual is positive:\n",
    "\n",
    "$$ \\hat{y} = -44.08 + 2.37 \\cdot 75 \\approx 133.90 $$\n",
    "$$ e = y - \\hat{y} = 170 - 133.90 = 36.1 > 0 $$\n",
    "\n",
    "Generally, if a prediction is below the actual data point (*underestimate*), then $e = y - \\hat{y} > 0$ — positive residual. On the other hand, if a prediction is above the actual data point (*overestimate*), then $e = y - \\hat{y} < 0$ — negative residual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4791a38",
   "metadata": {},
   "source": [
    "### Assumptions for the Least Squares Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c67cca",
   "metadata": {},
   "source": [
    "1. Linearity. The scatterplot must have a linear trend, and the corresponding residual plot should be random (no regular pattern). The simulation below shows: A linear trend in the data with a corresponding random residual plot without any pattern (left side of the figure below). A non-linear pattern in the data and residuals (right side of the figure below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26987ab2",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fce4fcc6",
   "metadata": {},
   "source": [
    "2. **Normal Residuals**. The residuals must be approximately normally distributed. Usually, it is very influential outliers that fail this condition and may completely invalidate the linear model. We discuss the classification of leverage and the influence of such outliers in more detail shortly. The figure below shows the data with dramatic outliers which destroy the correct linear trend in the regression line and show up as extreme right-skew outliers in the residual histogram below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bcc29b",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41447ce4",
   "metadata": {},
   "source": [
    "3. **Constant Variability**. The variability of observations about the least squares line should remain approximately the same. The figure below shows the data where the variability increases (**funnel pattern**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f35c37",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6919ac0",
   "metadata": {},
   "source": [
    "4. **Independent Observations**. The observations must be independent - one does not affect the other. The least squares line model cannot explain the time series data (sequential observations in time such as gasoline prices over several years or stock price variations in time). There are specialized time series analysis techniques for such data. The figure below shows the data with oscillating time dependence (seasonal changes on top of linear trend). In addition, the Durbin-Watson test can be used to check for autocorrelation. It reports a test statistic with a range from 0 to 4, where the value of 2 indicates that there is no autocorrelation. The (0,2) range corresponds to positive autocorrelation (common in time series data), and the (2,4) range indicates negative autocorrelation (less common in time series data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7e9dd5",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d3a8506",
   "metadata": {},
   "source": [
    "###  Statistical Analysis of Regression Coffcients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641dd6d4",
   "metadata": {},
   "source": [
    "Assuming the data set satisfies all the assumptions of the linear regression above, we can analyze the goodness of fit of the linear model in several ways: correlation, slope, ANOVA, etc. The correlation analysis was already done in the previous section.\n",
    "\n",
    "The slope hypothesis test:\n",
    "\n",
    "$H_0: \\beta_1 = 0$ - zero slope, no linear relationship\n",
    "\n",
    "$H_1: \\beta_1 \\ne 0$ - non-zero slope, there is a linear relationship\n",
    "\n",
    "The standard error of the slope is:\n",
    "\n",
    "$$SE(b_1) = \\sqrt{\\frac{1}{n-2} \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}i)^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}}$$\n",
    "\n",
    "The regression summaries for `WT` vs. `WAIST` are given in the extensive tables below and the figure shows the scatterplot with the best fit line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acdc971",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca16c741",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f99d651",
   "metadata": {},
   "source": [
    "The diagnostic plots above provide extremely important data summaries. The top-left one is Residuals vs. Fitted values. This plot checks if residuals have non-linear patterns. If you find equally spread residuals around a horizontal line without distinct patterns (as in this case), that is a good indication we don't have a non-linear relationship.\n",
    "The top-right plot shows a quantile-quantile (QQ) plot of standardized residuals vs. the standard normal distribution quantiles. If the residuals approximately follow a straight line (as in this case) the normality assumption of residuals holds.\n",
    "The bottom-left figure is the Scale-Location plot. It checks if residuals are spread equally along the ranges of predictors - assumption of equal variance (homoscedasticity). The top-left figure shows this too, but this one shows the possible funnel pattern more clearly. This example shows the line with equally (randomly) spread points, so the homoscedasticity holds.\n",
    "The bottom-right figure shows Residuals vs. Leverage. We discuss it in more detail in the next section.\n",
    "\n",
    "The \"Coef\" part of the table gives the y-intercept, slope, and the corresponding statistical analysis. For the slope:\n",
    "\n",
    "$$t = \\frac{\\text{point estimate} - \\text{null value}}{SE} = \\frac{b_1 - 0}{SE(b_1)} \\approx \\frac{2.37}{0.2} \\approx 11.96$$\n",
    "\n",
    "The t-distribution for the slope with $df = n - 2 = 40 - 2 = 38$ degrees of freedom produced an extremely low **p-value** $= 1.87 \\cdot 10^{-14} < 0.05$, so the null hypothesis of zero slope $\\beta_1 = 0$ can be rejected (significant linear model). Note also that the t-test and p-value are exactly the same as for the correlation hypothesis. It is not an accident because for simple one-variable regression $b_1 = \\frac{s_y r}{s_x}$, i.e., slope is proportional to the correlation coefficient.\n",
    "In addition, slope standard error can be used to define the confidence interval:\n",
    "\n",
    "$$\\text{point estimate} \\pm t_{df} \\cdot SE = b_1 \\pm t_{n-2} \\cdot SE(b_1)$$\n",
    "\n",
    "$$= 2.37 \\pm 2.024 \\cdot 0.2 = (1.97, 2.77)$$\n",
    "\n",
    "This confidence interval does not contain 0 which proves, yet again, that the slope is not 0 and the linear model is valid.\n",
    "\n",
    "The code output also shows an analogous statistical analysis for the y-intercept $H_0: \\beta_0 = 0$, which is rarely used (unless we look for direct proportionality). It is the output of the linear model when input is zero, but $x = 0$ is often impractical. For example, in our `WT` vs. `WAIST` example, `WAIST` = 0 is not practical and the resulting $b_0 = -44.08$ corresponds to negative weight which does not make sense either. We will see some other examples in future sections where $b_0$ has a practical meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5078c93",
   "metadata": {},
   "source": [
    "### Leverage and Influence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f9d5aa",
   "metadata": {},
   "source": [
    "Generally, outliers in regression are observations that fall far from the regression line. However, the leverage and influence of such outliers must be investigated.\n",
    "1. **Leverage**. Points that fall horizontally far away from the center (mean) of the data cloud pull harder on the line - high-leverage points, so leverage measures the extremity of the predictors. A high-leverage point is an observation with x-values extreme enough to *potentially* significantly change the slopes or trends in the fitted model. An outlier can have a high or low leverage.\n",
    "\n",
    "The formula for leverage is:\n",
    "\n",
    "$$h_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{s_x^2}$$\n",
    "\n",
    "where $n$ is the sample size of $(x, y)$ pairs, $x_i$ is the x-coordinate of a particular point, $\\bar{x}$ is the mean of all x's, and $s_x^2$ is the variance of x's.\n",
    "\n",
    "In general $1/n \\le h_i \\le 1$. If there are $p$ independent variables in the model, the mean value for leverage is $(p+1)/n$ (for simple regression $p=1$, so it is $2/n$). A rule of thumb (Steven's) is that values that are 3 times this mean value are considered large.\n",
    "\n",
    "2. **Influence**. An observation with high leverage that significantly affects the regression line is influential. The standard measure of influence is Cook's distance, which estimates the magnitude of the effect of deleting the $i$-th observation from the model. Cook's distance is defined in a more general context of multiple regression with $p$ explanatory variables (predictors), in our case $p = 1$.\n",
    "\n",
    "Let:\n",
    "\n",
    "$\\hat{y}_j$ = predicted mean response of observation $j$ for the model fitted with all $n$ observations.\n",
    "\n",
    "$\\hat{y}_j^{(-i)}$ = same response fitted without the $i$-th observation.\n",
    "\n",
    "$se$ = residual standard error. \n",
    "Then Cook's distance is:\n",
    "\n",
    "$$D_i = \\frac{\\sum_{j=1}^{n} (\\hat{y}_j - \\hat{y}_j^{(-i)})^2}{(p+1) s_e^2}$$\n",
    "\n",
    "The Residuals vs. Leverage diagnostic plot with Cook's distances shown as dashed level curves with levels 0.5, 1, etc. illustrates the interplay between leverage and influence. The figure below shows the effect of adding one point (big square) to the original data on the scatterplot and Residuals vs. Leverage plots. Highly influential points cross Cook's level curves as shown in the bottom-right plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3375a00",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cbc91dd",
   "metadata": {},
   "source": [
    "In the top plots of the above figure, the additional point has low leverage since its predictor value is close to $\\bar{x}$. Its inclusion only slightly modifies the y-intercept of the original line, not the slope, so it has very low influence, as can be seen in the top-right graph with none of the points crossing Cook's distance level 0.5.\n",
    "In the middle plots, the additional point is far apart from the original observations' mean, so it has high leverage. However, it conforms very well to the regression line model fitted without it. Its inclusion changes the regression line very slightly, so the influence is still low and it does not cross Cook's distance level 0.5.\n",
    "Lastly, in the bottom plots, the additional point is very far away from the original observations and has high leverage. This time, however, its inclusion significantly alters the regression line by turning it clockwise, so it has very high influence and it crosses Cook's distance level curves 0.5 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dace82",
   "metadata": {},
   "source": [
    "### Regression ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb11035",
   "metadata": {},
   "source": [
    "Next, we introduce an Analysis of Variance (ANOVA) for regression. It is studied in much more detail in the context of comparing several means, but it can illuminate the goodness of fit of the regression line as well. Assume there is a set of observations (not shown in the figure below) that produces the regression line $\\hat{y} = 9 + 1.5x$. Concentrate on just one data point $(2, 15)$ to keep the figure clear. Also, the overall mean of the response variable is $\\bar{y} = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3140aee6",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54089f72",
   "metadata": {},
   "source": [
    "The total vertical deviation of data point $(2, 15)$ from overall response mean is $y_i - \\bar{y} = 15 - 10 = 5$. It can be broken into the sum of two deviations:\n",
    "\n",
    "$$\\text{total deviation} = \\text{explained deviation} + \\text{unexplained deviation (residual)}$$\n",
    "\n",
    "$$y_i - \\bar{y} = (\\hat{y}_i - \\bar{y}) + (y_i - \\hat{y}_i)$$\n",
    "\n",
    "$$5 = 2 + 3$$\n",
    "\n",
    "As always, deviations are both positive and negative and would cancel each other out in a sum, so squared deviations are considered. The total sum of squares can be broken into:\n",
    "\n",
    "$$SST (\\text{total}) = SSR (\\text{explained or regression}) + SSE (\\text{residual or error})$$\n",
    "\n",
    "$$\\sum_{i=1}^n (y_i - \\bar{y})^2 = \\sum_{i=1}^n (\\hat{y}i - \\bar{y})^2 + \\sum{i=1}^n (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "The ANOVA results are best represented in a table below that shows degrees of freedom, sums of squares, averaged squared deviations (sum of squares divided by the corresponding degrees of freedom), F ratio of averaged deviations, and the corresponding p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc8fb26",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73d626bb",
   "metadata": {},
   "source": [
    "For **simple regression** with only one explanatory variable, the F-test ($F = t^2$, i.e., $143.12 \\approx 11.96^2$) and the p-value is the same as for the slope hypothesis t-test. Thus, this confirms, yet again, the validity of the linear model. This is not true for multiple regression where the F-test provides overall significance and each variable has its own t-test.\n",
    "Moreover, the averaged residual sum of squares\n",
    "$MME = \\frac{1}{df} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "in the ANOVA can be used to calculate the residual standard error:\n",
    "\n",
    "$$\\text{Residual standard error} = \\sqrt{MME} = \\sqrt{\\frac{1}{df} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n",
    "\n",
    "For `WT` vs. `WAIST`, the residual standard error is $\\sqrt{149.30} = 12.22$.\n",
    "\n",
    "When correlation was introduced, the **coefficient of determination** $r^2$ was defined, which measures the proportion of the variation in the response variable that is explained by the linear model. More precisely:\n",
    "\n",
    "$$r^2 = \\frac{SSR}{SST} = \\frac{21360.1}{21360.1 + 5671} = 0.79019$$\n",
    "\n",
    "which is the ratio of the explained variation in ANOVA to the total variation. It is exactly the $r^2$ that we obtained before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a80d5e",
   "metadata": {},
   "source": [
    "### Linear Model Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a05cb1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4b46f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02e4d457",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68a383",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0e015cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e886afa",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf8502c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6f71da",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7465e585",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b524c367",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e61f16c4",
   "metadata": {},
   "source": [
    "## Linear Model Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7f9986",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
